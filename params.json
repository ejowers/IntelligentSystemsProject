{
  "name": "Sign-to-Speech",
  "tagline": "Sign Language translator using a neural network and fuzzy string matching algorithms.",
  "body": "## Introduction\r\nMany deaf and mute people in America communicate primarily through American sign language (ASL). Unfortunately, most people who are not deaf or mute do not understand sign language. This causes a communication barrier for an entire community of people. The purpose of this project is to bridge this gap. The eventual goal is to take live video of a person signing and translate it into spoken words using image processing and neural network techniques, combined with word correction and text-to-speech algorithms. This project is a simplification of this idea. Images of ASL letters are used instead of video. Arrays of these images are translated into text. This project focuses on translating technical terms used by engineers in the industry.\r\n\r\n## Methods\r\nThe basic process used is as follows. An array of ASL letter images is passed into the program. Each image is processed and broken down into vectors containing a representation of each pixel. These vectors go through a neural network, which is trained to identify each sign. Once the signs have been translated, they are combined into a single word and sent through a fuzzy string search algorithm. This algorithm compares the word to a dictionary of known words using the Levenshtein distance to determine which word was likely intended.  This algorithm will also determine if the translated letters are intended to be a single word or a phrase of several words since there is no ASL sign used for a space. This should help to mitigate mistakes made by the neural network. Lastly, the translated word or phrase is printed to the screen and checked against the known result to calculate the system’s accuracy.\r\n\r\n## Algorithms\r\n### Image Processing\r\nThe image processing is fairly simple. The images are taken in and resized to a set width and height of 32 pixels. Then, they are flattened. This is a built-in function in OpenCV that converts the image into a single vector of size width x height x 3, totaling 3072 values representing the rgb values for each pixel.\r\n\r\n### Neural Network\r\nThe neural network has 26 possible outcomes, one for each letter of the alphabet. The neural network used is arranged with an input layer of 768 nodes, a hidden layer of 384 nodes and an output layer of 26 nodes. The output of this neural network is a vector with 26 values. All of these values will be zero, except one that will have a value of one. If the one is in the first position, the letter is “A”. If the one is on the second position, the letter is “B”, and so forth. The training dataset for the network is fairly small, so the neural network is trained 500 times with the same dataset. This project uses TensorFlow with the Keras API for the neural network.\r\n\r\n### Fuzzy String Search\r\nThere are many fuzzy string search algorithms. This project uses FuzzyWuzzy, a python library combining the functionalities of several other libraries into a simple API. Using FuzzyWuzzy, the word provided by the neural network is compared to a dictionary of known words. The Levenshtein distance is calculated for each word and the one with the shortest distance, or highest ratio of correct letters, is chosen as the intended word.\r\n\r\n## Results\r\nThe results of this project were positive. The neural network achieved 100% accuracy for the training data and 65% accuracy for the test data. The program has properly translated all 36 words in the given dictionary, including some that were purposefully misspelled. In sign language, there are many signs that look very similar to each other. These are especially confusing for the neural network to differentiate. The most difficult letters to tell apart are i and j. The letter j requires movement, but since this program uses images, i and j look identical. The other letters that are easily confused are a, m, n, s, and t because they are all a variation of a closed fist. This is where the fuzzy string search algorithm is especially useful. Many letters are never confused because they are unique compared to the other letter’s signs.\r\n\r\n## Future Work\r\nThis project is a step towards bridging the communication barrier between the ASL community and the non-ASL community, but it is only a small part of the eventual goal. First, datasets need to be obtained. Different people sign differently, so a larger set of signing images and videos are needed to fully train this neural network. Then, live video will replace the image arrays and the text-to-speech portion will be added onto the end of the process. Lastly, the neural network will need to be expanded to handle all signs, not just letters. This step will also include expanding the dictionary of known words to include all words. This will require a much bigger network and a higher level of fluency in American Sign Language. \r\n\r\n## References & Libraries\r\n1. PyImageSearch blog by Adrian Rosebrock for tutorials on computer vision and neural network software and techniques\r\n2. Shutterstock for ASL fingerspelling images\r\n3. StackOverflow for Python help\r\n4. Linux Mint MATE\r\n5. Python 2.7\r\n6. OpenCV\r\n7. Keras\r\n8. TensorFlow\r\n9. NumPy\r\n10. Sci-Kit Learning\r\n11. FuzzyWuzzy\r\n12. Atom\r\n\r\n## Contact\r\nEmily Jowers\r\nEmail: emi.jowers@gmail.com",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}